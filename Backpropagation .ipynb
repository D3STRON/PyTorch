{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac6f08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad= True)\n",
    "\n",
    "y_ = w*x\n",
    "\n",
    "loss = (y_ - y)**2\n",
    "\n",
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06ded6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5)= 0.000\n",
      "epoch 1:w = 1.600, b = 0.140, loss = 54.00000000\n",
      "epoch 2:w = 2.212, b = 0.197, loss = 8.37959957\n",
      "epoch 3:w = 2.445, b = 0.223, loss = 1.67620087\n",
      "epoch 4:w = 2.534, b = 0.236, loss = 0.68875295\n",
      "epoch 5:w = 2.566, b = 0.245, loss = 0.54085684\n",
      "epoch 6:w = 2.578, b = 0.251, loss = 0.51628745\n",
      "epoch 7:w = 2.581, b = 0.257, loss = 0.50985038\n",
      "epoch 8:w = 2.581, b = 0.263, loss = 0.50609434\n",
      "epoch 9:w = 2.580, b = 0.269, loss = 0.50275046\n",
      "epoch 10:w = 2.578, b = 0.275, loss = 0.49948555\n",
      "epoch 11:w = 2.576, b = 0.280, loss = 0.49624988\n",
      "epoch 12:w = 2.574, b = 0.286, loss = 0.49303660\n",
      "epoch 13:w = 2.573, b = 0.291, loss = 0.48984402\n",
      "epoch 14:w = 2.571, b = 0.297, loss = 0.48667246\n",
      "epoch 15:w = 2.569, b = 0.302, loss = 0.48352101\n",
      "epoch 16:w = 2.567, b = 0.308, loss = 0.48039040\n",
      "epoch 17:w = 2.565, b = 0.313, loss = 0.47727972\n",
      "epoch 18:w = 2.563, b = 0.319, loss = 0.47418940\n",
      "epoch 19:w = 2.562, b = 0.324, loss = 0.47111896\n",
      "epoch 20:w = 2.560, b = 0.330, loss = 0.46806836\n",
      "Prediction after training: f(5)= 13.129\n"
     ]
    }
   ],
   "source": [
    "# linear regression implmenetation using numpy\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([1,2,3,4], dtype = np.float32)\n",
    "Y = np.array([4,6,8,10], dtype = np.float32)\n",
    "\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred-y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE, J  = 1/N * (w*x + b - y)**2\n",
    "# dJ/dw = 1/N * 2 *(w*x + b - y)*x\n",
    "# dJ/dw = 1/N * 2 *(w*x + b - y)\n",
    "\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5)= {forward(5):.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Predriction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    db = gradient(1, Y, y_pred)\n",
    "    \n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch+1}:w = {w:.3f}, b = {b:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5)= {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7c18bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5)= 0.000\n",
      "epoch 1:w = 0.400, b = 0.140, loss = 54.00000000\n",
      "epoch 11:w = 1.503, b = 4.860, loss = 1.78267241\n",
      "epoch 21:w = 0.609, b = 7.566, loss = 7.11165619\n",
      "epoch 31:w = 0.358, b = 6.436, loss = 3.67926884\n",
      "epoch 41:w = 0.806, b = 4.466, loss = 2.19601917\n",
      "epoch 51:w = 1.298, b = 3.367, loss = 0.86372375\n",
      "epoch 61:w = 1.559, b = 3.020, loss = 0.27237505\n",
      "epoch 71:w = 1.663, b = 2.872, loss = 0.14877838\n",
      "epoch 81:w = 1.728, b = 2.687, loss = 0.09712599\n",
      "epoch 91:w = 1.794, b = 2.489, loss = 0.05715268\n",
      "epoch 101:w = 1.853, b = 2.339, loss = 0.02977762\n",
      "epoch 111:w = 1.896, b = 2.243, loss = 0.01470091\n",
      "epoch 121:w = 1.925, b = 2.181, loss = 0.00760492\n",
      "epoch 131:w = 1.944, b = 2.135, loss = 0.00415424\n",
      "epoch 141:w = 1.959, b = 2.099, loss = 0.00228116\n",
      "epoch 151:w = 1.970, b = 2.072, loss = 0.00122589\n",
      "epoch 161:w = 1.978, b = 2.052, loss = 0.00064818\n",
      "epoch 171:w = 1.984, b = 2.038, loss = 0.00034316\n",
      "epoch 181:w = 1.988, b = 2.028, loss = 0.00018333\n",
      "epoch 191:w = 1.991, b = 2.020, loss = 0.00009841\n",
      "epoch 201:w = 1.994, b = 2.015, loss = 0.00005273\n",
      "epoch 211:w = 1.995, b = 2.011, loss = 0.00002816\n",
      "epoch 221:w = 1.997, b = 2.008, loss = 0.00001502\n",
      "epoch 231:w = 1.998, b = 2.006, loss = 0.00000802\n",
      "epoch 241:w = 1.998, b = 2.004, loss = 0.00000429\n",
      "epoch 251:w = 1.999, b = 2.003, loss = 0.00000229\n",
      "epoch 261:w = 1.999, b = 2.002, loss = 0.00000122\n",
      "epoch 271:w = 1.999, b = 2.002, loss = 0.00000065\n",
      "epoch 281:w = 1.999, b = 2.001, loss = 0.00000035\n",
      "epoch 291:w = 2.000, b = 2.001, loss = 0.00000019\n",
      "Prediction after training: f(5)= 27.997\n"
     ]
    }
   ],
   "source": [
    "# linear regression implmenetation using torch\n",
    "import numpy as np\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "Y = torch.tensor([4,6,8,10], dtype = torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype= torch.float32, requires_grad= True)\n",
    "b = torch.tensor(0.0, dtype= torch.float32, requires_grad= True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred-y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE, J  = 1/N * (w*x + b - y)**2\n",
    "# dJ/dw = 1/N * 2 *(w*x +b - y)*x\n",
    "# dJ/db = 1/N * 2 *(w*x + b -y)\n",
    "\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5)= {forward(5):.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 300\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Predriction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients\n",
    "    l.backward()\n",
    "    \n",
    "    # since we do not want the below function to be a part of the computational graph \n",
    "    # for gradient calculation\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}:w = {w:.3f}, b = {b:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5)= {forward(13):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4140e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after training: f(5)= 41.995\n"
     ]
    }
   ],
   "source": [
    "# linear regression implmenetation using torch\n",
    "import numpy as np\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "Y = torch.tensor([4,6,8,10], dtype = torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype= torch.float32, requires_grad= True)\n",
    "b = torch.tensor(0.0, dtype= torch.float32, requires_grad= True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred-y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE, J  = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2 *(w*x - y)*x\n",
    "\n",
    "# def gradient(x, y, y_pred):\n",
    "#     return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5)= {forward(5):.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 300\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Predriction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients\n",
    "    l.backward()\n",
    "    \n",
    "    # since we do not want the below function to be a part of the computational graph \n",
    "    # for gradient calculation\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}:w = {w:.3f}, b = {b:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5)= {forward(13):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b912fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5)= 0.000\n",
      "epoch 1:w = 0.400, b = 0.140, loss = 54.00000000\n",
      "epoch 51:w = 2.345, b = 0.985, loss = 0.17315590\n",
      "epoch 101:w = 2.297, b = 1.126, loss = 0.12829719\n",
      "epoch 151:w = 2.256, b = 1.248, loss = 0.09506008\n",
      "epoch 201:w = 2.220, b = 1.353, loss = 0.07043342\n",
      "epoch 251:w = 2.190, b = 1.443, loss = 0.05218674\n",
      "epoch 301:w = 2.163, b = 1.520, loss = 0.03866710\n",
      "epoch 351:w = 2.140, b = 1.587, loss = 0.02864994\n",
      "epoch 401:w = 2.121, b = 1.645, loss = 0.02122775\n",
      "epoch 451:w = 2.104, b = 1.694, loss = 0.01572841\n",
      "epoch 501:w = 2.090, b = 1.737, loss = 0.01165373\n",
      "epoch 551:w = 2.077, b = 1.773, loss = 0.00863470\n",
      "epoch 601:w = 2.066, b = 1.805, loss = 0.00639778\n",
      "epoch 651:w = 2.057, b = 1.832, loss = 0.00474035\n",
      "epoch 701:w = 2.049, b = 1.855, loss = 0.00351228\n",
      "epoch 751:w = 2.042, b = 1.876, loss = 0.00260239\n",
      "epoch 801:w = 2.036, b = 1.893, loss = 0.00192822\n",
      "epoch 851:w = 2.031, b = 1.908, loss = 0.00142867\n",
      "epoch 901:w = 2.027, b = 1.921, loss = 0.00105858\n",
      "epoch 951:w = 2.023, b = 1.932, loss = 0.00078434\n",
      "Prediction after training: f(5)= 28.202\n"
     ]
    }
   ],
   "source": [
    "# implementation of linear regression using torch SGD optimizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "Y = torch.tensor([4,6,8,10], dtype = torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype= torch.float32, requires_grad= True)\n",
    "b = torch.tensor(0.0, dtype= torch.float32, requires_grad= True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "\n",
    "# gradient\n",
    "# MSE, J  = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2 *(w*x - y)*x\n",
    "\n",
    "# def gradient(x, y, y_pred):\n",
    "#     return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5)= {forward(5):.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 1000\n",
    "\n",
    "\n",
    "# def loss(y, y_pred):\n",
    "#     return ((y_pred-y)**2).mean()\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w,b], lr=learning_rate) \n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Predriction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients\n",
    "    l.backward()\n",
    "    \n",
    "    # since we do not want the below function to be a part of the computational graph \n",
    "    # for gradient calculation\n",
    "    optimizer.step()\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f'epoch {epoch+1}:w = {w:.3f}, b = {b:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5)= {forward(13):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8a36c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5)= -8.212\n",
      "epoch 1:w = -0.087, b = -0.258, loss = 88.21510315\n",
      "epoch 51:w = 2.406, b = 0.806, loss = 0.23968147\n",
      "epoch 101:w = 2.350, b = 0.972, loss = 0.17758822\n",
      "epoch 151:w = 2.301, b = 1.115, loss = 0.13158160\n",
      "epoch 201:w = 2.259, b = 1.238, loss = 0.09749377\n",
      "epoch 251:w = 2.223, b = 1.344, loss = 0.07223672\n",
      "epoch 301:w = 2.192, b = 1.436, loss = 0.05352296\n",
      "epoch 351:w = 2.165, b = 1.514, loss = 0.03965700\n",
      "epoch 401:w = 2.142, b = 1.582, loss = 0.02938343\n",
      "epoch 451:w = 2.122, b = 1.640, loss = 0.02177122\n",
      "epoch 501:w = 2.105, b = 1.690, loss = 0.01613114\n",
      "epoch 551:w = 2.091, b = 1.733, loss = 0.01195215\n",
      "epoch 601:w = 2.078, b = 1.770, loss = 0.00885577\n",
      "epoch 651:w = 2.067, b = 1.802, loss = 0.00656160\n",
      "epoch 701:w = 2.058, b = 1.830, loss = 0.00486172\n",
      "epoch 751:w = 2.050, b = 1.854, loss = 0.00360227\n",
      "epoch 801:w = 2.043, b = 1.874, loss = 0.00266906\n",
      "epoch 851:w = 2.037, b = 1.892, loss = 0.00197759\n",
      "epoch 901:w = 2.032, b = 1.907, loss = 0.00146528\n",
      "epoch 951:w = 2.027, b = 1.920, loss = 0.00108570\n",
      "Prediction after training: f(5)= 28.237\n"
     ]
    }
   ],
   "source": [
    "# implementation of linear regression using torch SGD optimizer and pytorch model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)\n",
    "Y = torch.tensor([[4],[6],[8],[10]], dtype = torch.float32)\n",
    "\n",
    "X_test = torch.tensor([13], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "# def forward(x):\n",
    "#     return w * x + b\n",
    "\n",
    "# gradient\n",
    "# MSE, J  = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2 *(w*x - y)*x\n",
    "\n",
    "# def gradient(x, y, y_pred):\n",
    "#     return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5)= {model.forward(X_test).item():.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 1000\n",
    "\n",
    "\n",
    "# def loss(y, y_pred):\n",
    "#     return ((y_pred-y)**2).mean()\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Predriction = forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients\n",
    "    l.backward()\n",
    "    \n",
    "    # since we do not want the below function to be a part of the computational graph \n",
    "    # for gradient calculation\n",
    "    optimizer.step()\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}:w = {w[0][0]:.3f}, b = {b[0]:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5)= {model.forward(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b1534bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss = 5765.6523\n",
      "epoch: 11, loss = 4269.3823\n",
      "epoch: 21, loss = 3187.9233\n",
      "epoch: 31, loss = 2405.3833\n",
      "epoch: 41, loss = 1838.5376\n",
      "epoch: 51, loss = 1427.5292\n",
      "epoch: 61, loss = 1129.2446\n",
      "epoch: 71, loss = 912.5865\n",
      "epoch: 81, loss = 755.0953\n",
      "epoch: 91, loss = 640.5316\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final linear regerssio using all the torch helper functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_numpy, Y_numpy = datasets.make_regression(n_samples= 100, n_features=1, noise=20, random_state=1)\n",
    "\n",
    "# print(Y_numpy)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
    "\n",
    "# below makes [1,2,3] to [[1],[2],[3]]\n",
    "Y = Y.view(Y.shape[0], 1)\n",
    "# print(Y)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Training Loops\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass\n",
    "    Y_pred = model(X)\n",
    "    # loss \n",
    "    loss = criterion(Y_pred, Y)\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "        \n",
    "# plot\n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_numpy, Y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020670d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
